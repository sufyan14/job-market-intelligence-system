# Job Market Intelligence System

## Overview
A data-driven system that analyzes job postings to identify in-demand skills,
salary trends, and skill gaps across technical roles.

## Problem Statement
Job seekers often lack structured insights into which skills are most valuable,
how experience affects compensation, and what learning paths improve employability.
This project addresses that gap using large-scale job posting data.

## Target Users
- Job seekers and students
- Early-career professionals
- Career switchers

## Data Sources
- LinkedIn job postings (primary dataset)
- Indeed job postings (secondary dataset)

## Core Features
- Skill demand analysis using NLP
- Job role clustering
- Salary prediction based on skills and experience
- Skill gap analysis for personalized recommendations

## Data Processing & NLP Pipeline

This project applies a structured NLP pipeline to transform raw job postings into
machine-readable features suitable for analysis and modeling.

## Skill Extraction

Technical skills are extracted from cleaned job descriptions using a curated
skill vocabulary. Extracted skills are standardized into canonical forms to
ensure consistency across sources.

### Output:

- Per-job skill lists
- Normalized skill vocabulary
- Aggregate skill frequency statistics

### Coverage:

- Skills are identified in ~53% of postings
- Lower coverage is expected for non-technical or generic roles

### Text Representation

Job descriptions are converted into numerical features using TF-IDF, enabling
downstream tasks such as clustering, similarity analysis, and prediction.

### Output:

- TF-IDF feature matrix
- Persisted vectorizer for reuse in modeling
- Generated Artifacts

The following artifacts are produced during processing:

**Canonical Dataset (v2)**
- Cleaned and standardized job records across sources

**Skills Dataset (v1)**
- Jobs enriched with normalized technical skills

**Skill Frequency Table**
- Skill counts and percentage coverage across the dataset

**NLP Artifacts**

- TF-IDF vectorizer (.pkl)
- Feature matrices (.pkl)

Note: Due to size constraints, large CSV outputs are not committed.
All datasets can be regenerated by running the notebooks in sequence.

### Reproducibility

To reproduce the full pipeline:

- Place raw datasets in data/raw/
- Run 01_dataset_validation_and_schema.ipynb
- Run 02_text_processing_and_nlp.ipynb

All processed datasets and artifacts will be generated locally.

## Tech Stack
- Python, Pandas, NumPy
- Scikit-learn
- spaCy / NLP
- FastAPI
- SQL
- Power BI (analytics)

## Project Status
This project is under active development.
